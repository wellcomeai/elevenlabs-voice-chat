"""
ElevenLabs Voice Assistant - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—á–∞—è –≤–µ—Ä—Å–∏—è
–ò—Å–ø—Ä–∞–≤–ª–µ–Ω–∏—è –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–∞–ª–∏–∑–∞ –ª–æ–≥–æ–≤ –∏ widget.js
"""

import asyncio
import json
import logging
import aiohttp
import websockets
from fastapi import FastAPI, WebSocket, WebSocketDisconnect
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.staticfiles import StaticFiles
from openai import OpenAI
import base64
import tempfile
import os
from pathlib import Path
import uuid

# –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

# API –∫–ª—é—á–∏ –∏–∑ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è - –ò–°–ü–†–ê–í–õ–ï–ù–´
ELEVENLABS_API_KEY = os.getenv("ELEVENLABS_API_KEY", "your_elevenlabs_key")
OPENAI_API_KEY = os.getenv("OPENAI_API_KEY", "your_openai_key")

# –í–ê–ñ–ù–û: –ò—Å–ø–æ–ª—å–∑—É–π—Ç–µ –≤–∞—à–∏ —Ä–µ–∞–ª—å–Ω—ã–µ –∫–ª—é—á–∏!
if ELEVENLABS_API_KEY == "your_elevenlabs_key" or not ELEVENLABS_API_KEY:
    logger.warning("‚ö†Ô∏è  ELEVENLABS_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è.")
    logger.warning("‚ö†Ô∏è  STT —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –æ—à–∏–±–∫–∞–º–∏!")

if OPENAI_API_KEY == "your_openai_key" or not OPENAI_API_KEY:
    logger.warning("‚ö†Ô∏è  OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω! –£—Å—Ç–∞–Ω–æ–≤–∏—Ç–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—É—é –æ–∫—Ä—É–∂–µ–Ω–∏—è.")
    logger.warning("‚ö†Ô∏è  LLM —Ñ—É–Ω–∫—Ü–∏—è –±—É–¥–µ—Ç —Ä–∞–±–æ—Ç–∞—Ç—å —Å –æ—à–∏–±–∫–∞–º–∏!")

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞
ASSISTANT_CONFIG = {
    "name": "–ê–ª–∏—Å–∞",
    "voice_id": "JBFqnCBsd6RMkjVDRZzb",  # Josh voice
    "model_id": "eleven_flash_v2_5",
    "system_prompt": """–¢—ã - –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–π –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –ø–æ –∏–º–µ–Ω–∏ –ê–ª–∏—Å–∞. 
–û—Ç–≤–µ—á–∞–π –∫—Ä–∞—Ç–∫–æ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è –º–∞–∫—Å–∏–º—É–º), –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ –∏ –ø–æ —Å—É—â–µ—Å—Ç–≤—É.""",
    "voice_settings": {
        "stability": 0.5,
        "similarity_boost": 0.8,
        "style": 0.2,
        "use_speaker_boost": True
    }
}

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è OpenAI –∫–ª–∏–µ–Ω—Ç–∞ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π
try:
    openai_client = OpenAI(
        api_key=OPENAI_API_KEY,
        timeout=30.0,
        max_retries=2
    )
    # –¢–µ—Å—Ç —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è
    test_response = openai_client.models.list()
    logger.info("‚úÖ OpenAI –∫–ª–∏–µ–Ω—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω –∏ –ø—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω")
except Exception as e:
    logger.error(f"‚ùå –û—à–∏–±–∫–∞ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏–∏ OpenAI: {e}")
    openai_client = None

# –°–æ–∑–¥–∞–Ω–∏–µ FastAPI –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
app = FastAPI(
    title="ElevenLabs Voice Assistant", 
    description="–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—á–∞—è –≤–µ—Ä—Å–∏—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞",
    version="2.1.0"
)

# HTML –∫–æ–Ω—Ç–µ–Ω—Ç - –û–ë–ù–û–í–õ–ï–ù–ù–´–ô —Å –ª—É—á—à–∏–º–∏ –ø—Ä–∞–∫—Ç–∏–∫–∞–º–∏ –∏–∑ widget.js
HTML_CONTENT = """<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Voice Assistant - –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –≤–µ—Ä—Å–∏—è</title>
    <style>
        * { margin: 0; padding: 0; box-sizing: border-box; }
        body {
            font-family: 'Inter', -apple-system, BlinkMacSystemFont, sans-serif;
            background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            padding: 20px;
        }
        .container {
            background: rgba(255, 255, 255, 0.95);
            border-radius: 24px;
            padding: 40px;
            max-width: 500px;
            width: 100%;
            text-align: center;
            box-shadow: 0 20px 60px rgba(0, 0, 0, 0.1);
        }
        .title {
            font-size: 28px;
            font-weight: 700;
            color: #2d3748;
            margin-bottom: 20px;
        }
        .voice-button {
            width: 120px;
            height: 120px;
            border-radius: 50%;
            border: none;
            background: linear-gradient(135deg, #4f46e5 0%, #7c3aed 100%);
            color: white;
            font-size: 28px;
            cursor: pointer;
            transition: all 0.3s ease;
            margin: 20px auto;
            display: flex;
            align-items: center;
            justify-content: center;
            box-shadow: 0 10px 30px rgba(79, 70, 229, 0.3);
        }
        .voice-button:hover { transform: scale(1.05); }
        .voice-button.recording {
            background: linear-gradient(135deg, #ef4444 0%, #dc2626 100%);
            animation: pulse 1.5s infinite;
        }
        .voice-button.processing {
            background: linear-gradient(135deg, #f59e0b 0%, #d97706 100%);
            animation: spin 1s linear infinite;
        }
        .voice-button.speaking {
            background: linear-gradient(135deg, #10b981 0%, #059669 100%);
            animation: wave 1s ease-in-out infinite;
        }
        @keyframes pulse { 0%, 100% { transform: scale(1); } 50% { transform: scale(1.1); } }
        @keyframes spin { from { transform: rotate(0deg); } to { transform: rotate(360deg); } }
        @keyframes wave { 0%, 100% { transform: scale(1); } 25% { transform: scale(1.05); } 75% { transform: scale(0.95); } }
        .status { margin-top: 20px; font-size: 16px; color: #475569; }
        .conversation {
            margin-top: 30px;
            padding: 20px;
            background: rgba(248, 250, 252, 0.8);
            border-radius: 16px;
            text-align: left;
            max-height: 300px;
            overflow-y: auto;
        }
        .message {
            margin-bottom: 10px;
            padding: 10px;
            border-radius: 8px;
            line-height: 1.4;
        }
        .message.user {
            background: #e0e7ff;
            color: #3730a3;
            margin-left: 20px;
        }
        .message.assistant {
            background: #f0fdf4;
            color: #166534;
            margin-right: 20px;
        }
        .error {
            color: #ef4444;
            background: rgba(254, 226, 226, 0.8);
            padding: 10px;
            border-radius: 8px;
            margin-top: 15px;
        }
    </style>
</head>
<body>
    <div class="container">
        <h1 class="title">üé§ –ì–æ–ª–æ—Å–æ–≤–æ–π –ê—Å—Å–∏—Å—Ç–µ–Ω—Ç</h1>
        <p>–ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è —Ä–∞–±–æ—á–∞—è –≤–µ—Ä—Å–∏—è</p>
        
        <button class="voice-button" id="voiceButton">
            <span id="buttonIcon">üé§</span>
        </button>
        
        <div class="status" id="statusText">–ù–∞–∂–º–∏—Ç–µ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞</div>
        
        <div class="conversation" id="conversation">
            <div style="text-align: center; color: #94a3b8; font-style: italic;">
                –ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞ –ø–æ—è–≤–∏—Ç—Å—è –∑–¥–µ—Å—å...
            </div>
        </div>
        
        <div class="error" id="errorMsg" style="display: none;"></div>
    </div>

    <script>
        class VoiceAssistant {
            constructor() {
                this.ws = null;
                this.mediaRecorder = null;
                this.audioChunks = [];
                this.isRecording = false;
                this.isProcessing = false;
                this.isSpeaking = false;
                this.audioQueue = [];
                this.currentAudio = null;
                
                this.initElements();
                this.connect();
                this.bindEvents();
            }
            
            initElements() {
                this.button = document.getElementById('voiceButton');
                this.icon = document.getElementById('buttonIcon');
                this.status = document.getElementById('statusText');
                this.conversation = document.getElementById('conversation');
                this.errorMsg = document.getElementById('errorMsg');
            }
            
            connect() {
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                const wsUrl = `${protocol}//${window.location.host}/ws/voice`;
                
                this.ws = new WebSocket(wsUrl);
                
                this.ws.onopen = () => {
                    console.log('WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ');
                    this.status.textContent = '–ì–æ—Ç–æ–≤! –ù–∞–∂–º–∏—Ç–µ –¥–ª—è –∑–∞–ø–∏—Å–∏';
                    this.hideError();
                };
                
                this.ws.onmessage = (event) => {
                    this.handleMessage(JSON.parse(event.data));
                };
                
                this.ws.onclose = () => {
                    console.log('WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–∫—Ä—ã—Ç–æ');
                    this.status.textContent = '–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—è–Ω–æ';
                    this.showError('–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —Å —Å–µ—Ä–≤–µ—Ä–æ–º –ø–æ—Ç–µ—Ä—è–Ω–æ. –û–±–Ω–æ–≤–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É.');
                };
                
                this.ws.onerror = (error) => {
                    console.error('WebSocket –æ—à–∏–±–∫–∞:', error);
                    this.showError('–û—à–∏–±–∫–∞ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏—è —Å —Å–µ—Ä–≤–µ—Ä–æ–º');
                };
            }
            
            bindEvents() {
                this.button.addEventListener('click', () => {
                    if (this.isSpeaking) {
                        this.stopSpeaking();
                    } else if (this.isRecording) {
                        this.stopRecording();
                    } else {
                        this.startRecording();
                    }
                });
            }
            
            async startRecording() {
                try {
                    this.hideError();
                    
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true,
                            sampleRate: 44100
                        }
                    });

                    this.mediaRecorder = new MediaRecorder(stream, {
                        mimeType: 'audio/webm;codecs=opus'
                    });

                    this.audioChunks = [];

                    this.mediaRecorder.ondataavailable = (event) => {
                        if (event.data.size > 0) {
                            this.audioChunks.push(event.data);
                        }
                    };

                    this.mediaRecorder.onstop = () => {
                        stream.getTracks().forEach(track => track.stop());
                        this.processAudio();
                    };

                    this.mediaRecorder.start();
                    this.isRecording = true;
                    this.updateUI();
                    this.status.textContent = '–ì–æ–≤–æ—Ä–∏—Ç–µ... –ù–∞–∂–º–∏—Ç–µ –¥–ª—è –æ—Å—Ç–∞–Ω–æ–≤–∫–∏';

                } catch (error) {
                    this.showError('–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–æ—Å—Ç—É–ø –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É: ' + error.message);
                }
            }
            
            stopRecording() {
                if (this.mediaRecorder && this.isRecording) {
                    this.mediaRecorder.stop();
                    this.isRecording = false;
                    this.isProcessing = true;
                    this.updateUI();
                    this.status.textContent = '–û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é...';
                }
            }
            
            async processAudio() {
                try {
                    const audioBlob = new Blob(this.audioChunks, { type: 'audio/webm' });
                    
                    if (this.ws && this.ws.readyState === WebSocket.OPEN) {
                        const reader = new FileReader();
                        reader.onload = () => {
                            const audioData = new Uint8Array(reader.result);
                            this.ws.send(JSON.stringify({
                                type: 'audio_data',
                                data: Array.from(audioData)
                            }));
                        };
                        reader.readAsArrayBuffer(audioBlob);
                    }

                } catch (error) {
                    this.showError('–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: ' + error.message);
                    this.resetState();
                }
            }
            
            handleMessage(data) {
                console.log('–ü–æ–ª—É—á–µ–Ω–æ:', data);

                switch (data.type) {
                    case 'transcription':
                        this.addMessage('user', data.text);
                        break;

                    case 'response':
                        this.addMessage('assistant', data.text);
                        break;

                    case 'tts_start':
                        this.isSpeaking = true;
                        this.updateUI();
                        this.status.textContent = '–ì–æ–≤–æ—Ä—é... –ù–∞–∂–º–∏—Ç–µ –¥–ª—è –ø—Ä–µ—Ä—ã–≤–∞–Ω–∏—è';
                        this.audioQueue = [];
                        break;

                    case 'audio_chunk':
                        this.audioQueue.push(data.audio);
                        if (this.audioQueue.length === 1) {
                            this.playAudioQueue();
                        }
                        break;

                    case 'tts_end':
                        setTimeout(() => {
                            if (!this.audioQueue.length) {
                                this.resetState();
                            }
                        }, 1000);
                        break;

                    case 'processing_complete':
                        this.isProcessing = false;
                        this.updateUI();
                        break;

                    case 'error':
                        this.showError(data.message);
                        this.resetState();
                        break;
                }
            }
            
            async playAudioQueue() {
                if (!this.audioQueue.length || !this.isSpeaking) return;

                try {
                    const audioData = this.audioQueue.shift();
                    const audioBlob = new Blob([
                        Uint8Array.from(atob(audioData), c => c.charCodeAt(0))
                    ], { type: 'audio/mpeg' });

                    const audioUrl = URL.createObjectURL(audioBlob);
                    this.currentAudio = new Audio(audioUrl);

                    this.currentAudio.onended = () => {
                        URL.revokeObjectURL(audioUrl);
                        if (this.audioQueue.length > 0) {
                            this.playAudioQueue();
                        } else if (!this.isSpeaking) {
                            this.resetState();
                        }
                    };

                    this.currentAudio.onerror = () => {
                        URL.revokeObjectURL(audioUrl);
                        console.error('–û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è –∞—É–¥–∏–æ');
                        this.playAudioQueue();
                    };

                    await this.currentAudio.play();

                } catch (error) {
                    console.error('–û—à–∏–±–∫–∞ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è:', error);
                    this.playAudioQueue();
                }
            }
            
            stopSpeaking() {
                this.isSpeaking = false;
                this.audioQueue = [];
                
                if (this.currentAudio) {
                    this.currentAudio.pause();
                    this.currentAudio.currentTime = 0;
                }
                
                this.resetState();
            }
            
            addMessage(role, text) {
                if (role === 'user' && this.conversation.innerHTML.includes('–ò—Å—Ç–æ—Ä–∏—è —Ä–∞–∑–≥–æ–≤–æ—Ä–∞')) {
                    this.conversation.innerHTML = '';
                }

                const messageDiv = document.createElement('div');
                messageDiv.className = `message ${role}`;
                messageDiv.innerHTML = `<strong>${role === 'user' ? '–í—ã:' : '–ê—Å—Å–∏—Å—Ç–µ–Ω—Ç:'}</strong> ${text}`;
                
                this.conversation.appendChild(messageDiv);
                this.conversation.scrollTop = this.conversation.scrollHeight;
            }
            
            updateUI() {
                this.button.className = 'voice-button';

                if (this.isRecording) {
                    this.button.classList.add('recording');
                    this.icon.textContent = '‚èπÔ∏è';
                } else if (this.isProcessing) {
                    this.button.classList.add('processing');
                    this.icon.textContent = '‚öôÔ∏è';
                } else if (this.isSpeaking) {
                    this.button.classList.add('speaking');
                    this.icon.textContent = 'üîä';
                } else {
                    this.icon.textContent = 'üé§';
                }
            }
            
            resetState() {
                this.isRecording = false;
                this.isProcessing = false;
                this.isSpeaking = false;
                this.updateUI();
                this.status.textContent = '–ì–æ—Ç–æ–≤! –ù–∞–∂–º–∏—Ç–µ –¥–ª—è –∑–∞–ø–∏—Å–∏';
            }
            
            showError(message) {
                this.errorMsg.textContent = message;
                this.errorMsg.style.display = 'block';
            }
            
            hideError() {
                this.errorMsg.style.display = 'none';
            }
        }

        document.addEventListener('DOMContentLoaded', () => {
            new VoiceAssistant();
        });
    </script>
</body>
</html>"""

class VoiceAssistantHandler:
    """–û–±—Ä–∞–±–æ—Ç—á–∏–∫ –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ - –ò–°–ü–†–ê–í–õ–ï–ù–ù–ê–Ø –≤–µ—Ä—Å–∏—è"""
    
    def __init__(self):
        self.conversation_history = []
        self.session_id = str(uuid.uuid4())
    
    async def speech_to_text(self, audio_data):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ä–µ—á–∏ –≤ —Ç–µ–∫—Å—Ç —á–µ—Ä–µ–∑ ElevenLabs - –ò–°–ü–†–ê–í–õ–ï–ù–û"""
        try:
            logger.info(f"[STT] –û–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ: {len(audio_data)} –±–∞–π—Ç")
            
            if len(audio_data) < 1000:
                logger.warning("[STT] –ê—É–¥–∏–æ —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ–µ")
                return "–ó–∞–ø–∏—Å—å —Å–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å –¥–æ–ª—å—à–µ."
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–∞—è —Ä–∞–±–æ—Ç–∞ —Å –≤—Ä–µ–º–µ–Ω–Ω—ã–º —Ñ–∞–π–ª–æ–º
            temp_file_path = None
            try:
                # –°–æ–∑–¥–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –∏ –∑–∞–ø–∏—Å—ã–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ
                with tempfile.NamedTemporaryFile(suffix='.webm', delete=False) as temp_file:
                    temp_file.write(audio_data)
                    temp_file_path = temp_file.name
                
                logger.info(f"[STT] –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —Å–æ–∑–¥–∞–Ω: {temp_file_path}")
                
                # –ü—Ä–æ–≤–µ—Ä—è–µ–º —á—Ç–æ —Ñ–∞–π–ª —Å—É—â–µ—Å—Ç–≤—É–µ—Ç –∏ –Ω–µ –ø—É—Å—Ç
                if not os.path.exists(temp_file_path):
                    logger.error("[STT] –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
                    return "–û—à–∏–±–∫–∞ —Å–æ–∑–¥–∞–Ω–∏—è –≤—Ä–µ–º–µ–Ω–Ω–æ–≥–æ —Ñ–∞–π–ª–∞"
                
                file_size = os.path.getsize(temp_file_path)
                logger.info(f"[STT] –†–∞–∑–º–µ—Ä —Ñ–∞–π–ª–∞: {file_size} –±–∞–π—Ç")
                
                if file_size == 0:
                    logger.error("[STT] –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª –ø—É—Å—Ç")
                    return "–û—à–∏–±–∫–∞: –ø—É—Å—Ç–æ–π —Ñ–∞–π–ª"
                
                url = "https://api.elevenlabs.io/v1/speech-to-text"
                headers = {'xi-api-key': ELEVENLABS_API_KEY}
                
                # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω–æ–µ —á—Ç–µ–Ω–∏–µ —Ñ–∞–π–ª–∞ –¥–ª—è FormData
                async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                    # –ß–∏—Ç–∞–µ–º –≤–µ—Å—å —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å –¥–ª—è FormData
                    with open(temp_file_path, 'rb') as audio_file:
                        audio_content = audio_file.read()
                    
                    logger.info(f"[STT] –ü—Ä–æ—á–∏—Ç–∞–Ω–æ –∏–∑ —Ñ–∞–π–ª–∞: {len(audio_content)} –±–∞–π—Ç")
                    
                    data = aiohttp.FormData()
                    data.add_field('audio', audio_content, 
                                 filename='audio.webm', 
                                 content_type='audio/webm')
                    data.add_field('model_id', 'eleven_multilingual_sts_v2')
                    
                    logger.info("[STT] –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –∑–∞–ø—Ä–æ—Å –∫ ElevenLabs...")
                    
                    async with session.post(url, data=data, headers=headers) as response:
                        logger.info(f"[STT] –û—Ç–≤–µ—Ç –æ—Ç ElevenLabs: {response.status}")
                        
                        if response.status == 200:
                            result = await response.json()
                            transcript = result.get('text', '').strip()
                            
                            if transcript and len(transcript) > 1:
                                logger.info(f"[STT] –£—Å–ø–µ—à–Ω–æ: {transcript}")
                                return transcript
                            else:
                                logger.warning("[STT] –ü—É—Å—Ç–æ–π —Ä–µ–∑—É–ª—å—Ç–∞—Ç")
                                return "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å."
                        else:
                            error_text = await response.text()
                            logger.error(f"[STT] –û—à–∏–±–∫–∞ {response.status}: {error_text}")
                            
                            # –ü–æ–ø—Ä–æ–±—É–µ–º –¥—Ä—É–≥—É—é –º–æ–¥–µ–ª—å
                            if response.status == 400:
                                return await self._try_alternative_stt(temp_file_path)
                            
                            return "–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
                        
            finally:
                # –û—á–∏—â–∞–µ–º –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª
                if temp_file_path and os.path.exists(temp_file_path):
                    try:
                        os.unlink(temp_file_path)
                        logger.info(f"[STT] –í—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª —É–¥–∞–ª–µ–Ω: {temp_file_path}")
                    except Exception as cleanup_error:
                        logger.warning(f"[STT] –ù–µ —É–¥–∞–ª–æ—Å—å —É–¥–∞–ª–∏—Ç—å –≤—Ä–µ–º–µ–Ω–Ω—ã–π —Ñ–∞–π–ª: {cleanup_error}")
                        
        except Exception as e:
            logger.error(f"[STT] –û–±—â–∞—è –æ—à–∏–±–∫–∞: {e}")
            return "–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏. –ü—Ä–æ–≤–µ—Ä—å—Ç–µ –ø–æ–¥–∫–ª—é—á–µ–Ω–∏–µ."
    
    async def _try_alternative_stt(self, file_path):
        """–ü–æ–ø—ã—Ç–∫–∞ —Å –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π –º–æ–¥–µ–ª—å—é STT"""
        try:
            logger.info("[STT ALT] –ü—Ä–æ–±—É–µ–º –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω—É—é –º–æ–¥–µ–ª—å")
            
            if not os.path.exists(file_path):
                logger.error("[STT ALT] –§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω")
                return "–§–∞–π–ª –Ω–µ –Ω–∞–π–¥–µ–Ω –¥–ª—è –∞–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–Ω–æ–π –æ–±—Ä–∞–±–æ—Ç–∫–∏"
            
            url = "https://api.elevenlabs.io/v1/speech-to-text"
            headers = {'xi-api-key': ELEVENLABS_API_KEY}
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=30)) as session:
                # –ß–∏—Ç–∞–µ–º —Ñ–∞–π–ª –≤ –ø–∞–º—è—Ç—å
                with open(file_path, 'rb') as audio_file:
                    audio_content = audio_file.read()
                
                logger.info(f"[STT ALT] –ü—Ä–æ—á–∏—Ç–∞–Ω–æ: {len(audio_content)} –±–∞–π—Ç")
                
                data = aiohttp.FormData()
                data.add_field('audio', audio_content, 
                             filename='audio.webm', 
                             content_type='audio/webm')
                data.add_field('model_id', 'eleven_english_sts_v2')
                
                async with session.post(url, data=data, headers=headers) as response:
                    logger.info(f"[STT ALT] –û—Ç–≤–µ—Ç: {response.status}")
                    
                    if response.status == 200:
                        result = await response.json()
                        transcript = result.get('text', '').strip()
                        
                        if transcript and len(transcript) > 1:
                            logger.info(f"[STT ALT] –£—Å–ø–µ—à–Ω–æ: {transcript}")
                            return transcript
                    else:
                        error_text = await response.text()
                        logger.error(f"[STT ALT] –û—à–∏–±–∫–∞ {response.status}: {error_text}")
            
            return "–ù–µ —É–¥–∞–ª–æ—Å—å —Ä–∞—Å–ø–æ–∑–Ω–∞—Ç—å —Ä–µ—á—å. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –≥–æ–≤–æ—Ä–∏—Ç—å —á–µ—Ç—á–µ."
            
        except Exception as e:
            logger.error(f"[STT ALT] –û—à–∏–±–∫–∞: {e}")
            return "–û—à–∏–±–∫–∞ —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Ä–µ—á–∏."
    
    async def generate_response(self, user_text):
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —á–µ—Ä–µ–∑ OpenAI GPT - –ò–°–ü–†–ê–í–õ–ï–ù–û"""
        try:
            if not openai_client:
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, —Å–µ—Ä–≤–∏—Å –≤—Ä–µ–º–µ–Ω–Ω–æ –Ω–µ–¥–æ—Å—Ç—É–ø–µ–Ω."
            
            logger.info(f"[LLM] –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç –¥–ª—è: {user_text}")
            
            self.conversation_history.append({"role": "user", "content": user_text})
            
            # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
            if len(self.conversation_history) > 10:
                self.conversation_history = self.conversation_history[-10:]
            
            messages = [
                {"role": "system", "content": ASSISTANT_CONFIG["system_prompt"]}
            ] + self.conversation_history
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∞–≤–∏–ª—å–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã –¥–ª—è –Ω–æ–≤–æ–π –≤–µ—Ä—Å–∏–∏ OpenAI API
            response = openai_client.chat.completions.create(
                model="gpt-4o-mini",  # –ò—Å–ø–æ–ª—å–∑—É–µ–º –±–æ–ª–µ–µ –¥–æ—Å—Ç—É–ø–Ω—É—é –º–æ–¥–µ–ª—å
                messages=messages,
                max_tokens=150,
                temperature=0.7,
                timeout=15
            )
            
            assistant_response = response.choices[0].message.content
            
            self.conversation_history.append({
                "role": "assistant", 
                "content": assistant_response
            })
            
            logger.info(f"[LLM] –û—Ç–≤–µ—Ç: {assistant_response}")
            return assistant_response
            
        except Exception as e:
            logger.error(f"[LLM] –û—à–∏–±–∫–∞: {e}")
            return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –ø—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞. –ü–æ–ø—Ä–æ–±—É–π—Ç–µ –µ—â–µ —Ä–∞–∑."
    
    async def text_to_speech_stream(self, text, websocket):
        """–ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞ –≤ —Ä–µ—á—å —á–µ—Ä–µ–∑ ElevenLabs - –ò–°–ü–†–ê–í–õ–ï–ù–û"""
        try:
            logger.info(f"[TTS] –ù–∞—á–∏–Ω–∞–µ–º —Å–∏–Ω—Ç–µ–∑ –¥–ª—è: {text[:50]}...")
            
            url = f"https://api.elevenlabs.io/v1/text-to-speech/{ASSISTANT_CONFIG['voice_id']}/stream"
            
            headers = {
                'xi-api-key': ELEVENLABS_API_KEY,
                'Content-Type': 'application/json',
                'Accept': 'audio/mpeg'  # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: —É–∫–∞–∑—ã–≤–∞–µ–º —Ñ–æ—Ä–º–∞—Ç
            }
            
            payload = {
                "text": text,
                "model_id": ASSISTANT_CONFIG["model_id"],
                "voice_settings": ASSISTANT_CONFIG["voice_settings"],
                "optimize_streaming_latency": 2,
                "output_format": "mp3_44100_128"
            }
            
            async with aiohttp.ClientSession(timeout=aiohttp.ClientTimeout(total=60)) as session:
                async with session.post(url, json=payload, headers=headers) as response:
                    if response.status == 200:
                        await websocket.send_json({"type": "tts_start", "text": text})
                        
                        chunk_count = 0
                        async for chunk in response.content.iter_chunked(2048):
                            if chunk:
                                chunk_count += 1
                                audio_b64 = base64.b64encode(chunk).decode('utf-8')
                                await websocket.send_json({
                                    "type": "audio_chunk",
                                    "audio": audio_b64
                                })
                        
                        await websocket.send_json({
                            "type": "tts_end",
                            "total_chunks": chunk_count
                        })
                        
                        logger.info(f"[TTS] –ó–∞–≤–µ—Ä—à–µ–Ω. –û—Ç–ø—Ä–∞–≤–ª–µ–Ω–æ {chunk_count} —á–∞–Ω–∫–æ–≤")
                    else:
                        error_text = await response.text()
                        logger.error(f"[TTS] –û—à–∏–±–∫–∞ {response.status}: {error_text}")
                        
                        # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –í–æ–∑–≤—Ä–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç–æ–≤—ã–π –æ—Ç–≤–µ—Ç –ø—Ä–∏ –æ—à–∏–±–∫–µ TTS
                        await websocket.send_json({
                            "type": "response",
                            "text": text  # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç –≤–º–µ—Å—Ç–æ –∞—É–¥–∏–æ
                        })
                        
                        await websocket.send_json({
                            "type": "processing_complete"
                        })
                        
        except Exception as e:
            logger.error(f"[TTS] –ò—Å–∫–ª—é—á–µ–Ω–∏–µ: {e}")
            
            # –ò–°–ü–†–ê–í–õ–ï–ù–ò–ï: –ü—Ä–∏ –æ—à–∏–±–∫–µ TTS –æ—Ç–ø—Ä–∞–≤–ª—è–µ–º —Ç–µ–∫—Å—Ç
            await websocket.send_json({
                "type": "response",
                "text": text
            })
            
            await websocket.send_json({
                "type": "processing_complete"
            })

@app.get("/")
async def get_main_page():
    """–ì–ª–∞–≤–Ω–∞—è —Å—Ç—Ä–∞–Ω–∏—Ü–∞"""
    return HTMLResponse(content=HTML_CONTENT)

@app.get("/health")
async def health_check():
    """Health check –¥–ª—è –ø—Ä–æ–≤–µ—Ä–∫–∏ —Å—Ç–∞—Ç—É—Å–∞"""
    elevenlabs_configured = ELEVENLABS_API_KEY and ELEVENLABS_API_KEY != "your_elevenlabs_key"
    openai_configured = OPENAI_API_KEY and OPENAI_API_KEY != "your_openai_key"
    
    return JSONResponse({
        "status": "healthy",
        "service": "Voice Assistant",
        "version": "2.1.0",
        "openai_status": "connected" if openai_client else "disconnected", 
        "elevenlabs_key": "configured" if elevenlabs_configured else "missing",
        "openai_key": "configured" if openai_configured else "missing",
        "issues": [
            "ElevenLabs API key not set" if not elevenlabs_configured else None,
            "OpenAI API key not set" if not openai_configured else None,
            "OpenAI client failed to initialize" if not openai_client else None
        ]
    })

@app.websocket("/ws/voice")
async def websocket_endpoint(websocket: WebSocket):
    """WebSocket endpoint –¥–ª—è –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –≤–∑–∞–∏–º–æ–¥–µ–π—Å—Ç–≤–∏—è - –ò–°–ü–†–ê–í–õ–ï–ù–û"""
    await websocket.accept()
    logger.info("[WS] WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–æ")
    
    handler = VoiceAssistantHandler()
    
    try:
        while True:
            message = await websocket.receive_json()
            
            if message["type"] == "audio_data":
                try:
                    audio_bytes = bytes(message["data"])
                    logger.info(f"[WS] –ü–æ–ª—É—á–µ–Ω—ã –∞—É–¥–∏–æ –¥–∞–Ω–Ω—ã–µ: {len(audio_bytes)} –±–∞–π—Ç")
                    
                    # STT
                    transcript = await handler.speech_to_text(audio_bytes)
                    
                    if transcript and transcript.strip() and not transcript.startswith("–û—à–∏–±–∫–∞") and not transcript.startswith("–ó–∞–ø–∏—Å—å"):
                        await websocket.send_json({
                            "type": "transcription",
                            "text": transcript
                        })
                        
                        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –æ—Ç–≤–µ—Ç
                        response = await handler.generate_response(transcript)
                        
                        await websocket.send_json({
                            "type": "response", 
                            "text": response
                        })
                        
                        # TTS
                        await handler.text_to_speech_stream(response, websocket)
                        
                        await websocket.send_json({
                            "type": "processing_complete"
                        })
                    else:
                        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—à–∏–±–∫—É
                        await websocket.send_json({
                            "type": "error",
                            "message": transcript
                        })
                        
                except Exception as e:
                    logger.error(f"[WS] –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: {e}")
                    await websocket.send_json({
                        "type": "error",
                        "message": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"
                    })
            
    except WebSocketDisconnect:
        logger.info("[WS] WebSocket —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –∑–∞–∫—Ä—ã—Ç–æ")
    except Exception as e:
        logger.error(f"[WS] WebSocket –æ—à–∏–±–∫–∞: {e}")

if __name__ == "__main__":
    import uvicorn
    port = int(os.getenv("PORT", 8000))
    
    # –ü—Ä–æ–≤–µ—Ä–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –ø–µ—Ä–µ–¥ –∑–∞–ø—É—Å–∫–æ–º
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Voice Assistant...")
    logger.info(f"üîë ElevenLabs API: {'‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω' if ELEVENLABS_API_KEY != 'your_elevenlabs_key' else '‚ùå –ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω'}")
    logger.info(f"üîë OpenAI API: {'‚úÖ –ù–∞—Å—Ç—Ä–æ–µ–Ω' if OPENAI_API_KEY != 'your_openai_key' else '‚ùå –ù–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω'}")
    logger.info(f"üåê –ó–∞–ø—É—Å–∫ –Ω–∞ –ø–æ—Ä—Ç—É: {port}")
    
    uvicorn.run(app, host="0.0.0.0", port=port, log_level="info")

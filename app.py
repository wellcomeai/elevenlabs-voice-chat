"""
üé§ Hands-Free Real-time Voice Assistant v6.0
–†–µ–∂–∏–º "—Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞" - –±–µ–∑ –∫–Ω–æ–ø–æ–∫, —Å –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏–µ–º

–û—Å–æ–±–µ–Ω–Ω–æ—Å—Ç–∏:
- –ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è –ø—Ä–æ—Å–ª—É—à–∫–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
- –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—á–∏ (VAD)
- –ü–µ—Ä–µ–±–∏–≤–∞–Ω–∏–µ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞ –≤–æ –≤—Ä–µ–º—è –≥–æ–≤–æ—Ä–µ–Ω–∏—è
- –ü–æ–¥–∞–≤–ª–µ–Ω–∏–µ —ç—Ö–∞ —Å–æ–±—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –≥–æ–ª–æ—Å–∞
- –£–º–Ω–æ–µ —É–ø—Ä–∞–≤–ª–µ–Ω–∏–µ –æ—á–µ—Ä–µ–¥—è–º–∏ –∞—É–¥–∏–æ
- –ë–µ—Å—à–æ–≤–Ω—ã–µ –ø–µ—Ä–µ—Ö–æ–¥—ã –º–µ–∂–¥—É —Å–æ—Å—Ç–æ—è–Ω–∏—è–º–∏
"""

import asyncio
import json
import logging
import base64
import time
import uuid
import os
import tempfile
import io
import wave
from typing import Optional, Dict, Any, List, Set
from dataclasses import dataclass
from enum import Enum
import threading
from concurrent.futures import ThreadPoolExecutor
import collections

from fastapi import FastAPI, WebSocket, WebSocketDisconnect, HTTPException
from fastapi.responses import HTMLResponse, JSONResponse
from fastapi.middleware.cors import CORSMiddleware
from openai import OpenAI
import uvicorn

# ===== –ö–û–ù–§–ò–ì–£–†–ê–¶–ò–Ø =====

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s | %(name)s | %(levelname)s | %(message)s'
)
logger = logging.getLogger(__name__)

OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
if not OPENAI_API_KEY:
    logger.error("üö® OPENAI_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω!")
    raise ValueError("OpenAI API key is required")

openai_client = OpenAI(api_key=OPENAI_API_KEY)

# –ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –¥–ª—è hands-free —Ä–µ–∂–∏–º–∞
HANDS_FREE_CONFIG = {
    # –ê—É–¥–∏–æ –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    "audio_chunk_duration_ms": 100,     # –ß–∞—Å—Ç—ã–µ —á–∞–Ω–∫–∏ –¥–ª—è –±—ã—Å—Ç—Ä–æ–π —Ä–µ–∞–∫—Ü–∏–∏
    "sample_rate": 16000,               # –°—Ç–∞–Ω–¥–∞—Ä—Ç–Ω—ã–π sample rate
    "channels": 1,                      # –ú–æ–Ω–æ
    
    # VAD –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    "vad_threshold": 0.008,             # –ü–æ—Ä–æ–≥ –¥–µ—Ç–µ–∫—Ü–∏–∏ –≥–æ–ª–æ—Å–∞
    "vad_hang_time_ms": 600,            # –í—Ä–µ–º—è "–≤–∏—Å–µ–Ω–∏—è" –ø–æ—Å–ª–µ —Ç–∏—à–∏–Ω—ã
    "vad_attack_time_ms": 150,          # –í—Ä–µ–º—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏
    "min_speech_duration_ms": 400,      # –ú–∏–Ω–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏
    "max_speech_duration_ms": 30000,    # –ú–∞–∫—Å–∏–º–∞–ª—å–Ω–∞—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å —Ä–µ—á–∏
    
    # –ü–µ—Ä–µ–±–∏–≤–∞–Ω–∏–µ
    "interrupt_threshold": 0.012,       # –ü–æ—Ä–æ–≥ –¥–ª—è –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è (–≤—ã—à–µ –æ–±—ã—á–Ω–æ–≥–æ)
    "interrupt_confirmation_ms": 200,   # –í—Ä–µ–º—è –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏—è –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è
    "interrupt_fade_out_ms": 100,       # –í—Ä–µ–º—è –∑–∞—Ç—É—Ö–∞–Ω–∏—è –ø—Ä–∏ –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏–∏
    
    # –≠—Ö–æ-–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
    "echo_suppression_duration_ms": 1500,  # –í—Ä–µ–º—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è —ç—Ö–∞ –ø–æ—Å–ª–µ TTS
    "echo_suppression_factor": 0.4,        # –§–∞–∫—Ç–æ—Ä —Å–Ω–∏–∂–µ–Ω–∏—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏
    
    # –ë—É—Ñ–µ—Ä–∏–∑–∞—Ü–∏—è
    "audio_buffer_size": 50,            # –†–∞–∑–º–µ—Ä –∫–æ–ª—å—Ü–µ–≤–æ–≥–æ –±—É—Ñ–µ—Ä–∞
    "processing_overlap_ms": 200,       # –ü–µ—Ä–µ–∫—Ä—ã—Ç–∏–µ –¥–ª—è –ø–ª–∞–≤–Ω–æ—Å—Ç–∏
    
    # OpenAI –Ω–∞—Å—Ç—Ä–æ–π–∫–∏
    "whisper": {
        "model": "whisper-1",
        "language": "ru",
        "temperature": 0.0,
        "prompt": "–†–∞–∑–≥–æ–≤–æ—Ä —Å –≥–æ–ª–æ—Å–æ–≤—ã–º –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–æ–º. –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç –ø–µ—Ä–µ–±–∏–≤–∞—Ç—å."
    },
    
    "gpt": {
        "model": "gpt-4o-mini",
        "max_tokens": 100,               # –ö–æ—Ä–æ—Ç–∫–∏–µ –æ—Ç–≤–µ—Ç—ã –¥–ª—è –¥–∏–∞–ª–æ–≥–∞
        "temperature": 0.8,              # –ë–æ–ª–µ–µ –∂–∏–≤–æ–π –¥–∏–∞–ª–æ–≥
        "stream": False
    },
    
    "tts": {
        "model": "tts-1",
        "voice": "alloy",
        "speed": 1.1,                    # –ß—É—Ç—å –±—ã—Å—Ç—Ä–µ–µ –¥–ª—è –¥–∏–Ω–∞–º–∏—á–Ω–æ—Å—Ç–∏
        "response_format": "mp3"
    }
}

# ===== –°–û–°–¢–û–Ø–ù–ò–Ø =====

class ConversationState(Enum):
    INITIALIZING = "initializing"       # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞
    LISTENING = "listening"             # –ê–∫—Ç–∏–≤–Ω–æ–µ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏–µ
    SPEECH_DETECTED = "speech_detected" # –û–±–Ω–∞—Ä—É–∂–µ–Ω–∞ —Ä–µ—á—å
    PROCESSING = "processing"           # –û–±—Ä–∞–±–æ—Ç–∫–∞ STT+LLM+TTS
    SPEAKING = "speaking"               # –í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –æ—Ç–≤–µ—Ç–∞
    INTERRUPTED = "interrupted"         # –ü–µ—Ä–µ–±–∏–≤–∞–Ω–∏–µ
    ERROR = "error"                     # –û—à–∏–±–∫–∞
    PAUSED = "paused"                   # –ü–∞—É–∑–∞

@dataclass
class AudioChunk:
    data: bytes
    timestamp: float
    amplitude: float
    chunk_id: int
    is_echo_suppressed: bool = False

@dataclass
class SpeechSegment:
    start_time: float
    end_time: float
    audio_data: bytes
    confidence: float = 0.0

# ===== –£–õ–£–ß–®–ï–ù–ù–´–ô VAD =====

class AdvancedVAD:
    """–ü—Ä–æ–¥–≤–∏–Ω—É—Ç—ã–π –¥–µ—Ç–µ–∫—Ç–æ—Ä –≥–æ–ª–æ—Å–æ–≤–æ–π –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –¥–ª—è hands-free —Ä–µ–∂–∏–º–∞"""
    
    def __init__(self, config: dict):
        self.threshold = config["vad_threshold"]
        self.hang_time_ms = config["vad_hang_time_ms"]
        self.attack_time_ms = config["vad_attack_time_ms"]
        self.interrupt_threshold = config["interrupt_threshold"]
        self.interrupt_confirmation_ms = config["interrupt_confirmation_ms"]
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ VAD
        self.is_speech_active = False
        self.speech_start_time = 0.0
        self.last_speech_time = 0.0
        self.potential_speech_start = 0.0
        
        # –î–ª—è –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è
        self.interrupt_candidate_start = 0.0
        self.is_interrupt_detected = False
        
        # –°–∫–æ–ª—å–∑—è—â–µ–µ –æ–∫–Ω–æ –¥–ª—è —Å–≥–ª–∞–∂–∏–≤–∞–Ω–∏—è
        self.amplitude_window = collections.deque(maxlen=5)
        self.long_term_noise = collections.deque(maxlen=50)  # –î–ª—è –∞–¥–∞–ø—Ç–∏–≤–Ω–æ–≥–æ –ø–æ—Ä–æ–≥–∞
        
        # –≠—Ö–æ-–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
        self.echo_suppression_until = 0.0
        self.echo_factor = config.get("echo_suppression_factor", 0.4)
        
    def set_echo_suppression(self, duration_ms: float):
        """–£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ—Ç –≤—Ä–µ–º—è –ø–æ–¥–∞–≤–ª–µ–Ω–∏—è —ç—Ö–∞"""
        self.echo_suppression_until = time.time() + (duration_ms / 1000.0)
        logger.debug(f"–≠—Ö–æ-–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ –Ω–∞ {duration_ms}ms")
        
    def process_chunk(self, chunk: AudioChunk) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –∞—É–¥–∏–æ —á–∞–Ω–∫ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç VAD —Ä–µ—à–µ–Ω–∏—è"""
        
        current_time = chunk.timestamp
        amplitude = chunk.amplitude
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –æ–∫–Ω–∞
        self.amplitude_window.append(amplitude)
        self.long_term_noise.append(amplitude)
        
        # –°–≥–ª–∞–∂–µ–Ω–Ω–∞—è –∞–º–ø–ª–∏—Ç—É–¥–∞
        smooth_amplitude = sum(self.amplitude_window) / len(self.amplitude_window)
        
        # –ê–¥–∞–ø—Ç–∏–≤–Ω—ã–π –ø–æ—Ä–æ–≥ –Ω–∞ –æ—Å–Ω–æ–≤–µ —Ñ–æ–Ω–æ–≤–æ–≥–æ —à—É–º–∞
        if len(self.long_term_noise) > 10:
            noise_floor = sum(sorted(self.long_term_noise)[:20]) / 20  # –ù–∏–∂–Ω–∏–µ 40%
            adaptive_threshold = max(self.threshold, noise_floor * 3)
        else:
            adaptive_threshold = self.threshold
        
        # –≠—Ö–æ-–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
        effective_amplitude = smooth_amplitude
        if current_time < self.echo_suppression_until:
            effective_amplitude *= self.echo_factor
            chunk.is_echo_suppressed = True
        
        result = {
            "timestamp": current_time,
            "amplitude": amplitude,
            "smooth_amplitude": smooth_amplitude,
            "effective_amplitude": effective_amplitude,
            "adaptive_threshold": adaptive_threshold,
            "is_speech_active": self.is_speech_active,
            "is_echo_suppressed": chunk.is_echo_suppressed
        }
        
        # –î–µ—Ç–µ–∫—Ü–∏—è —Ä–µ—á–∏
        has_voice = effective_amplitude > adaptive_threshold
        
        if has_voice:
            if not self.is_speech_active:
                # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –Ω–∞—á–∞–ª–æ —Ä–µ—á–∏
                if self.potential_speech_start == 0:
                    self.potential_speech_start = current_time
                    
                # –ü–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∏–µ —Ä–µ—á–∏ –ø–æ—Å–ª–µ attack time
                elif (current_time - self.potential_speech_start) * 1000 >= self.attack_time_ms:
                    self.is_speech_active = True
                    self.speech_start_time = self.potential_speech_start
                    result["speech_started"] = True
                    result["speech_start_time"] = self.speech_start_time
                    logger.info("üé§ –ù–∞—á–∞–ª–æ —Ä–µ—á–∏ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ")
                    
            self.last_speech_time = current_time
            
        else:
            # –¢–∏—à–∏–Ω–∞
            self.potential_speech_start = 0  # –°–±—Ä–æ—Å –ø–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –Ω–∞—á–∞–ª–∞
            
            if self.is_speech_active:
                silence_duration = (current_time - self.last_speech_time) * 1000
                
                if silence_duration >= self.hang_time_ms:
                    # –ö–æ–Ω–µ—Ü —Ä–µ—á–∏
                    self.is_speech_active = False
                    speech_duration = (self.last_speech_time - self.speech_start_time) * 1000
                    
                    result["speech_ended"] = True
                    result["speech_duration_ms"] = speech_duration
                    result["should_process"] = speech_duration >= HANDS_FREE_CONFIG["min_speech_duration_ms"]
                    
                    logger.info(f"üîá –ö–æ–Ω–µ—Ü —Ä–µ—á–∏. –î–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å: {speech_duration:.0f}ms")
        
        # –î–µ—Ç–µ–∫—Ü–∏—è –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è (—Ç–æ–ª—å–∫–æ –∫–æ–≥–¥–∞ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≥–æ–≤–æ—Ä–∏—Ç)
        if effective_amplitude > self.interrupt_threshold:
            if not self.is_interrupt_detected and self.interrupt_candidate_start == 0:
                self.interrupt_candidate_start = current_time
                
            elif self.interrupt_candidate_start > 0:
                interrupt_duration = (current_time - self.interrupt_candidate_start) * 1000
                if interrupt_duration >= self.interrupt_confirmation_ms:
                    self.is_interrupt_detected = True
                    result["interrupt_detected"] = True
                    result["interrupt_amplitude"] = effective_amplitude
                    logger.info("‚ö° –ü–µ—Ä–µ–±–∏–≤–∞–Ω–∏–µ –æ–±–Ω–∞—Ä—É–∂–µ–Ω–æ!")
        else:
            # –°–±—Ä–æ—Å –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è –ø—Ä–∏ —Å–Ω–∏–∂–µ–Ω–∏–∏ –∞–º–ø–ª–∏—Ç—É–¥—ã
            if self.interrupt_candidate_start > 0:
                self.interrupt_candidate_start = 0
                
        return result
    
    def reset_interrupt_detection(self):
        """–°–±—Ä–æ—Å —Å–æ—Å—Ç–æ—è–Ω–∏—è –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è"""
        self.is_interrupt_detected = False
        self.interrupt_candidate_start = 0

# ===== –ö–û–õ–¨–¶–ï–í–û–ô –ê–£–î–ò–û –ë–£–§–ï–† =====

class CircularAudioBuffer:
    """–ö–æ–ª—å—Ü–µ–≤–æ–π –±—É—Ñ–µ—Ä –¥–ª—è –ø–æ—Å—Ç–æ—è–Ω–Ω–æ–≥–æ —Ö—Ä–∞–Ω–µ–Ω–∏—è –∞—É–¥–∏–æ"""
    
    def __init__(self, max_size: int = 50):
        self.buffer = collections.deque(maxlen=max_size)
        self.speech_start_index = None
        self.total_chunks = 0
        
    def add_chunk(self, chunk: AudioChunk):
        """–î–æ–±–∞–≤–ª—è–µ—Ç —á–∞–Ω–∫ –≤ –±—É—Ñ–µ—Ä"""
        chunk.chunk_id = self.total_chunks
        self.buffer.append(chunk)
        self.total_chunks += 1
        
    def mark_speech_start(self, timestamp: float):
        """–û—Ç–º–µ—á–∞–µ—Ç –Ω–∞—á–∞–ª–æ —Ä–µ—á–∏ –≤ –±—É—Ñ–µ—Ä–µ"""
        # –ù–∞—Ö–æ–¥–∏–º –±–ª–∏–∂–∞–π—à–∏–π —á–∞–Ω–∫ –∫ –≤—Ä–µ–º–µ–Ω–∏ –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏
        for i, chunk in enumerate(self.buffer):
            if abs(chunk.timestamp - timestamp) < 0.1:  # 100ms tolerance
                self.speech_start_index = len(self.buffer) - len(self.buffer) + i
                break
        
    def extract_speech_segment(self, end_timestamp: float) -> Optional[bytes]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç —Å–µ–≥–º–µ–Ω—Ç —Ä–µ—á–∏ –∏–∑ –±—É—Ñ–µ—Ä–∞"""
        if self.speech_start_index is None:
            return None
            
        speech_chunks = []
        speech_started = False
        
        for chunk in self.buffer:
            # –ù–∞—á–∏–Ω–∞–µ–º —Å–±–æ—Ä —Å –æ—Ç–º–µ—á–µ–Ω–Ω–æ–≥–æ –Ω–∞—á–∞–ª–∞ —Ä–µ—á–∏
            if not speech_started and chunk.timestamp >= (self.buffer[0].timestamp if self.speech_start_index == 0 else self.buffer[self.speech_start_index].timestamp):
                speech_started = True
                
            if speech_started:
                speech_chunks.append(chunk.data)
                
                # –ó–∞–∫–∞–Ω—á–∏–≤–∞–µ–º –Ω–∞ —É–∫–∞–∑–∞–Ω–Ω–æ–º –≤—Ä–µ–º–µ–Ω–∏
                if chunk.timestamp >= end_timestamp:
                    break
        
        if speech_chunks:
            return b''.join(speech_chunks)
        return None
    
    def clear_speech_markers(self):
        """–û—á–∏—â–∞–µ—Ç –º–∞—Ä–∫–µ—Ä—ã —Ä–µ—á–∏"""
        self.speech_start_index = None

# ===== HANDS-FREE –°–ï–°–°–ò–Ø =====

class HandsFreeSession:
    """–û—Å–Ω–æ–≤–Ω–∞—è —Å–µ—Å—Å–∏—è hands-free –≥–æ–ª–æ—Å–æ–≤–æ–≥–æ –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç–∞"""
    
    def __init__(self, session_id: str, websocket: WebSocket):
        self.session_id = session_id
        self.websocket = websocket
        self.state = ConversationState.INITIALIZING
        
        # –ö–æ–º–ø–æ–Ω–µ–Ω—Ç—ã
        self.vad = AdvancedVAD(HANDS_FREE_CONFIG)
        self.audio_buffer = CircularAudioBuffer(HANDS_FREE_CONFIG["audio_buffer_size"])
        
        # –û–±—Ä–∞–±–æ—Ç–∫–∞
        self.executor = ThreadPoolExecutor(max_workers=3, thread_name_prefix="OpenAI")
        self.current_processing_task = None
        self.current_playback_task = None
        
        # –°–æ—Å—Ç–æ—è–Ω–∏–µ
        self.conversation_history = []
        self.is_active = True
        self.last_interaction = time.time()
        
        # –ú–µ—Ç—Ä–∏–∫–∏
        self.total_exchanges = 0
        self.interruptions_count = 0
        self.false_positives = 0
        
        logger.info(f"üé§ –°–æ–∑–¥–∞–Ω–∞ hands-free —Å–µ—Å—Å–∏—è: {session_id}")
    
    async def initialize(self):
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —Å–µ—Å—Å–∏–∏"""
        await self._update_state(ConversationState.LISTENING)
        await self._send_event("session_ready", {
            "message": "–ì–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≥–æ—Ç–æ–≤. –ì–æ–≤–æ—Ä–∏—Ç–µ –≤ –ª—é–±–æ–µ –≤—Ä–µ–º—è!",
            "config": {
                "vad_threshold": HANDS_FREE_CONFIG["vad_threshold"],
                "interrupt_enabled": True,
                "echo_suppression": True
            }
        })
        logger.info("‚úÖ Hands-free —Å–µ—Å—Å–∏—è –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞")
    
    async def process_audio_chunk(self, audio_data: bytes):
        """–ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ —á–∞–Ω–∫–æ–≤"""
        
        if not self.is_active:
            return
            
        try:
            # –°–æ–∑–¥–∞–µ–º —á–∞–Ω–∫
            timestamp = time.time()
            amplitude = self._calculate_amplitude(audio_data)
            
            chunk = AudioChunk(
                data=audio_data,
                timestamp=timestamp,
                amplitude=amplitude,
                chunk_id=0  # –ë—É–¥–µ—Ç —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ –±—É—Ñ–µ—Ä–µ
            )
            
            # –î–æ–±–∞–≤–ª—è–µ–º –≤ –∫–æ–ª—å—Ü–µ–≤–æ–π –±—É—Ñ–µ—Ä
            self.audio_buffer.add_chunk(chunk)
            
            # VAD –∞–Ω–∞–ª–∏–∑
            vad_result = self.vad.process_chunk(chunk)
            
            # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º VAD —Å–æ–±—ã—Ç–∏—è
            await self._handle_vad_events(vad_result)
            
            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ (–∫–∞–∂–¥—ã–µ 10 —á–∞–Ω–∫–æ–≤ –¥–ª—è —ç–∫–æ–Ω–æ–º–∏–∏)
            if chunk.chunk_id % 10 == 0:
                await self._send_audio_status(vad_result)
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∞—É–¥–∏–æ: {e}")
            await self._send_event("error", {"message": str(e)})
    
    async def _handle_vad_events(self, vad_result: Dict[str, Any]):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Å–æ–±—ã—Ç–∏–π VAD"""
        
        # –ù–∞—á–∞–ª–æ —Ä–µ—á–∏
        if vad_result.get("speech_started"):
            if self.state == ConversationState.LISTENING:
                await self._update_state(ConversationState.SPEECH_DETECTED)
                self.audio_buffer.mark_speech_start(vad_result["speech_start_time"])
                await self._send_event("speech_detection", {
                    "type": "started",
                    "timestamp": vad_result["speech_start_time"]
                })
            
            elif self.state == ConversationState.SPEAKING:
                # –ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–µ –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏–µ
                logger.info("üé§ –†–µ—á—å –æ–±–Ω–∞—Ä—É–∂–µ–Ω–∞ –≤–æ –≤—Ä–µ–º—è –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏—è")
        
        # –ö–æ–Ω–µ—Ü —Ä–µ—á–∏
        if vad_result.get("speech_ended"):
            if self.state == ConversationState.SPEECH_DETECTED:
                speech_duration = vad_result["speech_duration_ms"]
                should_process = vad_result["should_process"]
                
                await self._send_event("speech_detection", {
                    "type": "ended",
                    "duration_ms": speech_duration,
                    "will_process": should_process
                })
                
                if should_process:
                    # –ò–∑–≤–ª–µ–∫–∞–µ–º —Ä–µ—á–µ–≤–æ–π —Å–µ–≥–º–µ–Ω—Ç –∏ –∑–∞–ø—É—Å–∫–∞–µ–º –æ–±—Ä–∞–±–æ—Ç–∫—É
                    speech_audio = self.audio_buffer.extract_speech_segment(vad_result["timestamp"])
                    if speech_audio:
                        await self._process_speech_segment(speech_audio, speech_duration)
                    else:
                        logger.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –∏–∑–≤–ª–µ—á—å —Ä–µ—á–µ–≤–æ–π —Å–µ–≥–º–µ–Ω—Ç")
                        await self._update_state(ConversationState.LISTENING)
                else:
                    # –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–∞—è —Ä–µ—á—å
                    self.false_positives += 1
                    await self._update_state(ConversationState.LISTENING)
                
                self.audio_buffer.clear_speech_markers()
        
        # –ü–µ—Ä–µ–±–∏–≤–∞–Ω–∏–µ
        if vad_result.get("interrupt_detected"):
            if self.state == ConversationState.SPEAKING:
                await self._handle_interruption(vad_result)
    
    async def _process_speech_segment(self, audio_data: bytes, duration_ms: float):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ—á–µ–≤–æ–≥–æ —Å–µ–≥–º–µ–Ω—Ç–∞ —á–µ—Ä–µ–∑ STT->LLM->TTS"""
        
        await self._update_state(ConversationState.PROCESSING)
        
        # –û—Ç–º–µ–Ω—è–µ–º –ø—Ä–µ–¥—ã–¥—É—â—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É –µ—Å–ª–∏ –µ—Å—Ç—å
        if self.current_processing_task and not self.current_processing_task.done():
            self.current_processing_task.cancel()
        
        self.current_processing_task = asyncio.create_task(
            self._full_processing_pipeline(audio_data, duration_ms)
        )
    
    async def _full_processing_pipeline(self, audio_data: bytes, duration_ms: float):
        """–ü–æ–ª–Ω—ã–π pipeline –æ–±—Ä–∞–±–æ—Ç–∫–∏"""
        
        try:
            pipeline_start = time.time()
            
            # 1. STT
            await self._send_event("processing_stage", {"stage": "stt", "status": "started"})
            transcript = await self._run_stt(audio_data)
            
            if not transcript or len(transcript.strip()) < 2:
                logger.info("–ü—É—Å—Ç–∞—è —Ç—Ä–∞–Ω—Å–∫—Ä–∏–ø—Ü–∏—è, –≤–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—é")
                await self._update_state(ConversationState.LISTENING)
                return
            
            await self._send_event("transcription", {"text": transcript})
            
            # 2. LLM
            await self._send_event("processing_stage", {"stage": "llm", "status": "started"})
            response_text = await self._run_llm(transcript)
            
            await self._send_event("llm_response", {"text": response_text})
            
            # 3. TTS –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
            await self._send_event("processing_stage", {"stage": "tts", "status": "started"})
            await self._run_tts_and_play(response_text)
            
            # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
            total_latency = (time.time() - pipeline_start) * 1000
            self.total_exchanges += 1
            
            await self._send_event("processing_complete", {
                "total_latency_ms": total_latency,
                "total_exchanges": self.total_exchanges
            })
            
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º—Å—è –∫ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—é
            await self._update_state(ConversationState.LISTENING)
            
        except asyncio.CancelledError:
            logger.info("–û–±—Ä–∞–±–æ—Ç–∫–∞ –æ—Ç–º–µ–Ω–µ–Ω–∞")
            await self._update_state(ConversationState.LISTENING)
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –≤ pipeline: {e}")
            await self._send_event("error", {"message": f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏: {str(e)}"})
            await self._update_state(ConversationState.LISTENING)
    
    async def _run_stt(self, audio_data: bytes) -> str:
        """–ó–∞–ø—É—Å–∫ Whisper STT"""
        
        def whisper_call():
            try:
                # –ö–æ–Ω–≤–µ—Ä—Ç–∏—Ä—É–µ–º –≤ WAV
                wav_data = self._convert_to_wav(audio_data)
                
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as temp_file:
                    temp_file.write(wav_data)
                    temp_path = temp_file.name
                
                try:
                    with open(temp_path, 'rb') as audio_file:
                        response = openai_client.audio.transcriptions.create(
                            model=HANDS_FREE_CONFIG["whisper"]["model"],
                            file=audio_file,
                            language=HANDS_FREE_CONFIG["whisper"]["language"],
                            temperature=HANDS_FREE_CONFIG["whisper"]["temperature"],
                            prompt=HANDS_FREE_CONFIG["whisper"]["prompt"]
                        )
                    return response.text.strip()
                finally:
                    try:
                        os.unlink(temp_path)
                    except:
                        pass
                        
            except Exception as e:
                logger.error(f"Whisper –æ—à–∏–±–∫–∞: {e}")
                return ""
        
        return await asyncio.get_event_loop().run_in_executor(self.executor, whisper_call)
    
    async def _run_llm(self, user_text: str) -> str:
        """–ó–∞–ø—É—Å–∫ GPT"""
        
        # –û–±–Ω–æ–≤–ª—è–µ–º –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history.append({"role": "user", "content": user_text})
        
        # –û–≥—Ä–∞–Ω–∏—á–∏–≤–∞–µ–º –∏—Å—Ç–æ—Ä–∏—é
        if len(self.conversation_history) > 8:
            self.conversation_history = self.conversation_history[-6:]
        
        def gpt_call():
            try:
                messages = [
                    {
                        "role": "system",
                        "content": """–¢—ã –≥–æ–ª–æ—Å–æ–≤–æ–π –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç –≤ —Ä–µ–∂–∏–º–µ –∂–∏–≤–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞.

–ü—Ä–∞–≤–∏–ª–∞:
- –û—Ç–≤–µ—á–∞–π –û–ß–ï–ù–¨ –∫—Ä–∞—Ç–∫–æ (1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è)
- –ì–æ–≤–æ—Ä–∏ –µ—Å—Ç–µ—Å—Ç–≤–µ–Ω–Ω–æ, –∫–∞–∫ –≤ —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–º —Ä–∞–∑–≥–æ–≤–æ—Ä–µ
- –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å –º–æ–∂–µ—Ç —Ç–µ–±—è –ø–µ—Ä–µ–±–∏–≤–∞—Ç—å - —ç—Ç–æ –Ω–æ—Ä–º–∞–ª—å–Ω–æ
- –ï—Å–ª–∏ —Ç–µ–±—è –ø–µ—Ä–µ–±–∏–ª–∏, –Ω–µ –æ–±–∏–∂–∞–π—Å—è, –ø—Ä–æ–¥–æ–ª–∂–∞–π –¥–∏–∞–ª–æ–≥
- –ë—É–¥—å –¥—Ä—É–∂–µ–ª—é–±–Ω—ã–º –∏ –æ—Ç–∑—ã–≤—á–∏–≤—ã–º
- –ù–µ –ø–æ–≤—Ç–æ—Ä—è–π "–∫–∞–∫ –¥–µ–ª–∞" –ø–æ—Å—Ç–æ—è–Ω–Ω–æ

–°—Ç–∏–ª—å: –ñ–∏–≤–æ–π —Ä–∞–∑–≥–æ–≤–æ—Ä, –∞ –Ω–µ —Ñ–æ—Ä–º–∞–ª—å–Ω—ã–µ –æ—Ç–≤–µ—Ç—ã."""
                    }
                ] + self.conversation_history
                
                response = openai_client.chat.completions.create(
                    model=HANDS_FREE_CONFIG["gpt"]["model"],
                    messages=messages,
                    max_tokens=HANDS_FREE_CONFIG["gpt"]["max_tokens"],
                    temperature=HANDS_FREE_CONFIG["gpt"]["temperature"]
                )
                
                return response.choices[0].message.content.strip()
                
            except Exception as e:
                logger.error(f"GPT –æ—à–∏–±–∫–∞: {e}")
                return "–ò–∑–≤–∏–Ω–∏—Ç–µ, –Ω–µ —Ä–∞—Å—Å–ª—ã—à–∞–ª. –ü–æ–≤—Ç–æ—Ä–∏—Ç–µ, –ø–æ–∂–∞–ª—É–π—Å—Ç–∞."
        
        response = await asyncio.get_event_loop().run_in_executor(self.executor, gpt_call)
        
        # –î–æ–±–∞–≤–ª—è–µ–º –æ—Ç–≤–µ—Ç –≤ –∏—Å—Ç–æ—Ä–∏—é
        self.conversation_history.append({"role": "assistant", "content": response})
        
        return response
    
    async def _run_tts_and_play(self, text: str):
        """TTS –∏ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Å –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç—å—é –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è"""
        
        await self._update_state(ConversationState.SPEAKING)
        
        # –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º —ç—Ö–æ-–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ
        self.vad.set_echo_suppression(HANDS_FREE_CONFIG["echo_suppression_duration_ms"])
        
        # –°–±—Ä–æ—Å –¥–µ—Ç–µ–∫—Ü–∏–∏ –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è
        self.vad.reset_interrupt_detection()
        
        def tts_call():
            try:
                with tempfile.NamedTemporaryFile(suffix='.mp3', delete=False) as temp_file:
                    temp_path = temp_file.name
                
                response = openai_client.audio.speech.create(
                    model=HANDS_FREE_CONFIG["tts"]["model"],
                    voice=HANDS_FREE_CONFIG["tts"]["voice"],
                    input=text,
                    speed=HANDS_FREE_CONFIG["tts"]["speed"],
                    response_format=HANDS_FREE_CONFIG["tts"]["response_format"]
                )
                
                response.stream_to_file(temp_path)
                
                with open(temp_path, 'rb') as audio_file:
                    audio_content = audio_file.read()
                
                try:
                    os.unlink(temp_path)
                except:
                    pass
                    
                return audio_content
                
            except Exception as e:
                logger.error(f"TTS –æ—à–∏–±–∫–∞: {e}")
                return b""
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º TTS
        audio_content = await asyncio.get_event_loop().run_in_executor(self.executor, tts_call)
        
        if audio_content and self.state == ConversationState.SPEAKING:
            # –ü–æ—Ç–æ–∫–æ–≤–æ–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
            await self._stream_audio_with_interruption(audio_content)
    
    async def _stream_audio_with_interruption(self, audio_content: bytes):
        """–ü–æ—Ç–æ–∫–æ–≤–æ–µ –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ —Å –ø—Ä–æ–≤–µ—Ä–∫–æ–π –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è"""
        
        chunk_size = 4096
        total_chunks = (len(audio_content) + chunk_size - 1) // chunk_size
        
        await self._send_event("tts_start", {
            "total_chunks": total_chunks,
            "total_size": len(audio_content)
        })
        
        for i in range(0, len(audio_content), chunk_size):
            # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å–æ—Å—Ç–æ—è–Ω–∏–µ - –º–æ–≥–ª–∏ –ø–µ—Ä–µ–±–∏—Ç—å
            if self.state != ConversationState.SPEAKING:
                logger.info("–í–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ –ø—Ä–µ—Ä–≤–∞–Ω–æ")
                break
                
            chunk = audio_content[i:i + chunk_size]
            chunk_id = i // chunk_size + 1
            
            audio_b64 = base64.b64encode(chunk).decode('utf-8')
            
            await self._send_event("audio_chunk", {
                "audio": audio_b64,
                "chunk_id": chunk_id,
                "total_chunks": total_chunks,
                "is_final": chunk_id == total_chunks
            })
            
            # –ù–µ–±–æ–ª—å—à–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É —á–∞–Ω–∫–∞–º–∏
            await asyncio.sleep(0.02)
        
        if self.state == ConversationState.SPEAKING:
            await self._send_event("tts_complete", {"interrupted": False})
    
    async def _handle_interruption(self, vad_result: Dict[str, Any]):
        """–û–±—Ä–∞–±–æ—Ç–∫–∞ –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º"""
        
        logger.info("‚ö° –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏–µ")
        self.interruptions_count += 1
        
        # –û—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –≤–æ—Å–ø—Ä–æ–∏–∑–≤–µ–¥–µ–Ω–∏–µ
        if self.current_playback_task and not self.current_playback_task.done():
            self.current_playback_task.cancel()
        
        await self._update_state(ConversationState.INTERRUPTED)
        
        await self._send_event("interrupted", {
            "timestamp": vad_result["timestamp"],
            "amplitude": vad_result.get("interrupt_amplitude", 0),
            "interruptions_count": self.interruptions_count
        })
        
        # –ë—ã—Å—Ç—Ä–æ –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ –ø—Ä–æ—Å–ª—É—à–∏–≤–∞–Ω–∏—é –Ω–æ–≤–æ–≥–æ —Å–æ–æ–±—â–µ–Ω–∏—è
        await asyncio.sleep(0.1)
        await self._update_state(ConversationState.LISTENING)
        
        # –°–±—Ä–æ—Å —ç—Ö–æ-–ø–æ–¥–∞–≤–ª–µ–Ω–∏—è –ø—Ä–∏ –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏–∏
        self.vad.echo_suppression_until = 0
    
    async def pause_session(self):
        """–ü—Ä–∏–æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–µ—Å—Å–∏–∏"""
        self.is_active = False
        await self._update_state(ConversationState.PAUSED)
        await self._send_event("session_paused", {"timestamp": time.time()})
    
    async def resume_session(self):
        """–í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–µ—Å—Å–∏–∏"""
        self.is_active = True
        await self._update_state(ConversationState.LISTENING)
        await self._send_event("session_resumed", {"timestamp": time.time()})
    
    async def close(self):
        """–ó–∞–∫—Ä—ã—Ç–∏–µ —Å–µ—Å—Å–∏–∏"""
        self.is_active = False
        
        # –û—Ç–º–µ–Ω—è–µ–º –∞–∫—Ç–∏–≤–Ω—ã–µ –∑–∞–¥–∞—á–∏
        if self.current_processing_task and not self.current_processing_task.done():
            self.current_processing_task.cancel()
        if self.current_playback_task and not self.current_playback_task.done():
            self.current_playback_task.cancel()
        
        self.executor.shutdown(wait=False)
        
        # –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞
        await self._send_event("session_stats", {
            "total_exchanges": self.total_exchanges,
            "interruptions_count": self.interruptions_count,
            "false_positives": self.false_positives,
            "session_duration": time.time() - self.last_interaction
        })
        
        logger.info(f"üîö Hands-free —Å–µ—Å—Å–∏—è –∑–∞–∫—Ä—ã—Ç–∞: {self.session_id}")
    
    def _calculate_amplitude(self, audio_data: bytes) -> float:
        """–í—ã—á–∏—Å–ª–µ–Ω–∏–µ –∞–º–ø–ª–∏—Ç—É–¥—ã –∞—É–¥–∏–æ"""
        if len(audio_data) < 2:
            return 0.0
            
        try:
            # –ü—Ä–µ–¥–ø–æ–ª–∞–≥–∞–µ–º 16-bit PCM
            sample_count = len(audio_data) // 2
            if sample_count == 0:
                return 0.0
                
            samples = [int.from_bytes(audio_data[i:i+2], byteorder='little', signed=True) 
                      for i in range(0, len(audio_data), 2)]
            
            rms = (sum(s * s for s in samples) / len(samples)) ** 0.5
            return rms / 32768.0  # –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∫ 0-1
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ —Ä–∞—Å—á–µ—Ç–∞ –∞–º–ø–ª–∏—Ç—É–¥—ã: {e}")
            return 0.0
    
    def _convert_to_wav(self, audio_data: bytes) -> bytes:
        """–ö–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏—è –∞—É–¥–∏–æ –≤ WAV —Ñ–æ—Ä–º–∞—Ç"""
        try:
            # –ï—Å–ª–∏ —É–∂–µ WAV - –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
            if audio_data.startswith(b'RIFF') and b'WAVE' in audio_data[:20]:
                return audio_data
            
            # –°–æ–∑–¥–∞–µ–º WAV –∏–∑ —Å—ã—Ä—ã—Ö –¥–∞–Ω–Ω—ã—Ö
            sample_rate = HANDS_FREE_CONFIG["sample_rate"]
            channels = HANDS_FREE_CONFIG["channels"]
            
            wav_buffer = io.BytesIO()
            
            with wave.open(wav_buffer, 'wb') as wav_file:
                wav_file.setnchannels(channels)
                wav_file.setsampwidth(2)  # 16-bit
                wav_file.setframerate(sample_rate)
                wav_file.writeframes(audio_data)
            
            return wav_buffer.getvalue()
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∫–æ–Ω–≤–µ—Ä—Ç–∞—Ü–∏–∏ –≤ WAV: {e}")
            return audio_data
    
    async def _update_state(self, new_state: ConversationState):
        """–û–±–Ω–æ–≤–ª–µ–Ω–∏–µ —Å–æ—Å—Ç–æ—è–Ω–∏—è"""
        old_state = self.state
        self.state = new_state
        
        await self._send_event("state_changed", {
            "old_state": old_state.value,
            "new_state": new_state.value,
            "timestamp": time.time()
        })
        
        logger.debug(f"üîÑ –°–æ—Å—Ç–æ—è–Ω–∏–µ: {old_state.value} ‚Üí {new_state.value}")
    
    async def _send_event(self, event_type: str, data: Dict[str, Any] = None):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å–æ–±—ã—Ç–∏—è –∫–ª–∏–µ–Ω—Ç—É"""
        try:
            message = {
                "type": event_type,
                "session_id": self.session_id,
                "timestamp": time.time()
            }
            
            if data:
                message.update(data)
            
            await self.websocket.send_json(message)
            
        except Exception as e:
            logger.warning(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —Å–æ–±—ã—Ç–∏—è {event_type}: {e}")
    
    async def _send_audio_status(self, vad_result: Dict[str, Any]):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —Å—Ç–∞—Ç—É—Å–∞ –∞—É–¥–∏–æ (–ø–µ—Ä–∏–æ–¥–∏—á–µ—Å–∫–∏)"""
        await self._send_event("audio_status", {
            "amplitude": vad_result.get("amplitude", 0),
            "smooth_amplitude": vad_result.get("smooth_amplitude", 0),
            "is_speech_active": vad_result.get("is_speech_active", False),
            "is_echo_suppressed": vad_result.get("is_echo_suppressed", False),
            "state": self.state.value
        })

# ===== SESSION MANAGER =====

class HandsFreeSessionManager:
    """–ú–µ–Ω–µ–¥–∂–µ—Ä hands-free —Å–µ—Å—Å–∏–π"""
    
    def __init__(self):
        self.sessions: Dict[str, HandsFreeSession] = {}
    
    async def create_session(self, websocket: WebSocket) -> HandsFreeSession:
        session_id = str(uuid.uuid4())
        session = HandsFreeSession(session_id, websocket)
        self.sessions[session_id] = session
        
        await session.initialize()
        
        logger.info(f"‚úÖ –°–æ–∑–¥–∞–Ω–∞ hands-free —Å–µ—Å—Å–∏—è: {session_id}")
        return session
    
    async def close_session(self, session_id: str):
        if session_id in self.sessions:
            await self.sessions[session_id].close()
            del self.sessions[session_id]
            logger.info(f"üóëÔ∏è –£–¥–∞–ª–µ–Ω–∞ —Å–µ—Å—Å–∏—è: {session_id}")
    
    def get_active_sessions_count(self) -> int:
        return len([s for s in self.sessions.values() if s.is_active])

# ===== FASTAPI APPLICATION =====

app = FastAPI(
    title="Hands-Free Voice Assistant v6.0",
    description="Real-time voice assistant —Å —Ä–µ–∂–∏–º–æ–º —Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞",
    version="6.0.0"
)

app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_credentials=True,
    allow_methods=["*"],
    allow_headers=["*"],
)

session_manager = HandsFreeSessionManager()

@app.get("/")
async def get_homepage():
    return HTMLResponse(content="""
<!DOCTYPE html>
<html lang="ru">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Hands-Free Voice Assistant v6.0</title>
    <style>
        body {
            font-family: 'Inter', system-ui, sans-serif;
            background: linear-gradient(135deg, #1e3c72 0%, #2a5298 100%);
            margin: 0;
            min-height: 100vh;
            display: flex;
            justify-content: center;
            align-items: center;
            color: white;
            overflow: hidden;
        }
        
        .container {
            text-align: center;
            background: rgba(255,255,255,0.1);
            padding: 3rem;
            border-radius: 30px;
            backdrop-filter: blur(20px);
            border: 1px solid rgba(255,255,255,0.2);
            max-width: 500px;
            width: 100%;
        }
        
        h1 {
            font-size: 2.8rem;
            margin-bottom: 0.5rem;
            background: linear-gradient(45deg, #fff, #a8c8ff);
            -webkit-background-clip: text;
            -webkit-text-fill-color: transparent;
        }
        
        .subtitle {
            font-size: 1.1rem;
            margin-bottom: 2rem;
            opacity: 0.9;
        }
        
        .main-button {
            width: 160px;
            height: 160px;
            border-radius: 50%;
            border: none;
            background: linear-gradient(135deg, #ff6b6b, #ee5a24);
            color: white;
            font-size: 3.5rem;
            cursor: pointer;
            margin: 1.5rem auto;
            display: block;
            transition: all 0.4s ease;
            box-shadow: 0 20px 40px rgba(0,0,0,0.3);
            position: relative;
            overflow: hidden;
        }
        
        .main-button:hover {
            transform: scale(1.05);
            box-shadow: 0 25px 50px rgba(0,0,0,0.4);
        }
        
        .main-button.active {
            background: linear-gradient(135deg, #74b9ff, #0984e3);
            animation: pulse-active 2s infinite;
        }
        
        .main-button.speaking {
            background: linear-gradient(135deg, #00b894, #00a085);
            animation: wave-speaking 0.8s ease-in-out infinite;
        }
        
        .main-button.processing {
            background: linear-gradient(135deg, #fdcb6e, #e17055);
            animation: spin-processing 1.2s linear infinite;
        }
        
        @keyframes pulse-active {
            0%, 100% { transform: scale(1); box-shadow: 0 20px 40px rgba(116, 185, 255, 0.4); }
            50% { transform: scale(1.08); box-shadow: 0 30px 60px rgba(116, 185, 255, 0.6); }
        }
        
        @keyframes wave-speaking {
            0%, 100% { transform: scale(1); }
            25% { transform: scale(1.05); }
            75% { transform: scale(0.95); }
        }
        
        @keyframes spin-processing {
            from { transform: rotate(0deg); }
            to { transform: rotate(360deg); }
        }
        
        .status {
            font-size: 1.3rem;
            margin: 1.5rem 0;
            min-height: 1.6rem;
            font-weight: 500;
        }
        
        .audio-visualizer {
            height: 60px;
            background: rgba(255,255,255,0.1);
            border-radius: 30px;
            margin: 2rem 0;
            position: relative;
            overflow: hidden;
        }
        
        .audio-bar {
            height: 100%;
            background: linear-gradient(90deg, #74b9ff, #0984e3, #74b9ff);
            width: 0%;
            border-radius: 30px;
            transition: width 0.1s ease;
            position: relative;
        }
        
        .audio-bar::after {
            content: '';
            position: absolute;
            top: 0;
            left: -100%;
            width: 100%;
            height: 100%;
            background: linear-gradient(90deg, transparent, rgba(255,255,255,0.3), transparent);
            animation: shine 2s infinite;
        }
        
        @keyframes shine {
            0% { left: -100%; }
            100% { left: 100%; }
        }
        
        .stats {
            margin-top: 2rem;
            display: grid;
            grid-template-columns: 1fr 1fr;
            gap: 1rem;
            font-size: 0.9rem;
        }
        
        .stat-item {
            background: rgba(255,255,255,0.1);
            padding: 1rem;
            border-radius: 15px;
            backdrop-filter: blur(10px);
        }
        
        .stat-value {
            font-size: 1.4rem;
            font-weight: 700;
            color: #74b9ff;
        }
        
        .controls {
            margin: 1.5rem 0;
            display: flex;
            gap: 0.5rem;
            justify-content: center;
        }
        
        .control-btn {
            padding: 0.5rem 1rem;
            border: none;
            border-radius: 20px;
            background: rgba(255,255,255,0.2);
            color: white;
            cursor: pointer;
            font-size: 0.8rem;
            transition: all 0.3s;
        }
        
        .control-btn:hover {
            background: rgba(255,255,255,0.3);
        }
        
        .control-btn.active {
            background: #74b9ff;
        }
        
        .conversation {
            margin-top: 2rem;
            max-height: 200px;
            overflow-y: auto;
            text-align: left;
            background: rgba(0,0,0,0.2);
            border-radius: 15px;
            padding: 1rem;
        }
        
        .message {
            margin: 0.5rem 0;
            padding: 0.5rem;
            border-radius: 8px;
            font-size: 0.9rem;
        }
        
        .message.user {
            background: rgba(116, 185, 255, 0.3);
            text-align: right;
        }
        
        .message.assistant {
            background: rgba(0, 184, 148, 0.3);
        }
        
        .connection-indicator {
            position: absolute;
            top: 20px;
            right: 20px;
            width: 12px;
            height: 12px;
            border-radius: 50%;
            background: #ef4444;
            transition: all 0.3s;
        }
        
        .connection-indicator.connected {
            background: #10b981;
            box-shadow: 0 0 15px rgba(16, 185, 129, 0.6);
        }
    </style>
</head>
<body>
    <div class="container">
        <div class="connection-indicator" id="connectionStatus"></div>
        
        <h1>üé§ –ê–ª–∏—Å–∞ Hands-Free</h1>
        <p class="subtitle">–†–∞–∑–≥–æ–≤–æ—Ä –±–µ–∑ –∫–Ω–æ–ø–æ–∫ ‚Ä¢ –ü–µ—Ä–µ–±–∏–≤–∞–Ω–∏–µ ‚Ä¢ –†–µ–∞–ª—å–Ω–æ–µ –≤—Ä–µ–º—è</p>
        
        <button class="main-button" id="mainButton">üìû</button>
        
        <div class="status" id="status">–ù–∞–∂–º–∏—Ç–µ —á—Ç–æ–±—ã –Ω–∞—á–∞—Ç—å —Ä–∞–∑–≥–æ–≤–æ—Ä</div>
        
        <div class="audio-visualizer">
            <div class="audio-bar" id="audioBar"></div>
        </div>
        
        <div class="controls">
            <button class="control-btn" id="pauseBtn">‚è∏Ô∏è –ü–∞—É–∑–∞</button>
            <button class="control-btn" id="muteBtn">üîá –¢–∏—à–∏–Ω–∞</button>
            <button class="control-btn active" id="autoBtn">ü§ñ –ê–≤—Ç–æ</button>
        </div>
        
        <div class="stats">
            <div class="stat-item">
                <div class="stat-value" id="exchangeCount">0</div>
                <div>–û–±–º–µ–Ω–æ–≤</div>
            </div>
            <div class="stat-item">
                <div class="stat-value" id="interruptCount">0</div>
                <div>–ü–µ—Ä–µ–±–∏–≤–∞–Ω–∏–π</div>
            </div>
        </div>
        
        <div class="conversation" id="conversation">
            <div style="text-align: center; opacity: 0.7; font-style: italic;">
                –ù–∞—á–Ω–∏—Ç–µ —Ä–∞–∑–≥–æ–≤–æ—Ä...
            </div>
        </div>
    </div>

    <script>
        class HandsFreeVoiceAssistant {
            constructor() {
                this.ws = null;
                this.mediaRecorder = null;
                this.isActive = false;
                this.isPaused = false;
                this.currentState = 'initializing';
                
                this.totalExchanges = 0;
                this.totalInterruptions = 0;
                
                this.initElements();
                this.connectWebSocket();
            }
            
            initElements() {
                this.mainButton = document.getElementById('mainButton');
                this.status = document.getElementById('status');
                this.connectionStatus = document.getElementById('connectionStatus');
                this.audioBar = document.getElementById('audioBar');
                this.conversation = document.getElementById('conversation');
                this.exchangeCount = document.getElementById('exchangeCount');
                this.interruptCount = document.getElementById('interruptCount');
                
                this.pauseBtn = document.getElementById('pauseBtn');
                this.muteBtn = document.getElementById('muteBtn');
                this.autoBtn = document.getElementById('autoBtn');
                
                // –°–æ–±—ã—Ç–∏—è
                this.mainButton.addEventListener('click', () => this.toggleSession());
                this.pauseBtn.addEventListener('click', () => this.togglePause());
                this.muteBtn.addEventListener('click', () => this.toggleMute());
            }
            
            connectWebSocket() {
                const protocol = window.location.protocol === 'https:' ? 'wss:' : 'ws:';
                const wsUrl = `${protocol}//${window.location.host}/ws/hands-free`;
                
                this.ws = new WebSocket(wsUrl);
                
                this.ws.onopen = () => {
                    console.log('üîó –ü–æ–¥–∫–ª—é—á–µ–Ω–æ –∫ hands-free –∞—Å—Å–∏—Å—Ç–µ–Ω—Ç—É');
                    this.connectionStatus.classList.add('connected');
                    this.status.textContent = '–ü–æ–¥–∫–ª—é—á–µ–Ω–æ! –ù–∞–∂–º–∏—Ç–µ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞';
                };
                
                this.ws.onmessage = (event) => {
                    const data = JSON.parse(event.data);
                    this.handleMessage(data);
                };
                
                this.ws.onclose = () => {
                    console.log('‚ùå –°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—è–Ω–æ');
                    this.connectionStatus.classList.remove('connected');
                    this.status.textContent = '–°–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –ø–æ—Ç–µ—Ä—è–Ω–æ - –ø–µ—Ä–µ–∑–∞–≥—Ä—É–∑–∏—Ç–µ —Å—Ç—Ä–∞–Ω–∏—Ü—É';
                    this.isActive = false;
                    this.updateUI();
                };
            }
            
            async toggleSession() {
                if (this.isActive) {
                    await this.stopSession();
                } else {
                    await this.startSession();
                }
            }
            
            async startSession() {
                try {
                    const stream = await navigator.mediaDevices.getUserMedia({
                        audio: {
                            sampleRate: 16000,
                            channelCount: 1,
                            echoCancellation: true,
                            noiseSuppression: true,
                            autoGainControl: true
                        }
                    });
                    
                    this.mediaRecorder = new MediaRecorder(stream, {
                        mimeType: 'audio/webm;codecs=opus',
                        audioBitsPerSecond: 16000
                    });
                    
                    this.mediaRecorder.ondataavailable = (event) => {
                        if (event.data.size > 0 && this.isActive) {
                            this.sendAudioChunk(event.data);
                        }
                    };
                    
                    this.mediaRecorder.start(100); // 100ms —á–∞–Ω–∫–∏
                    this.isActive = true;
                    this.updateUI();
                    
                } catch (error) {
                    console.error('–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É:', error);
                    this.status.textContent = '–û—à–∏–±–∫–∞ –¥–æ—Å—Ç—É–ø–∞ –∫ –º–∏–∫—Ä–æ—Ñ–æ–Ω—É: ' + error.message;
                }
            }
            
            async stopSession() {
                this.isActive = false;
                
                if (this.mediaRecorder) {
                    this.mediaRecorder.stop();
                    this.mediaRecorder.stream.getTracks().forEach(track => track.stop());
                    this.mediaRecorder = null;
                }
                
                if (this.ws && this.ws.readyState === WebSocket.OPEN) {
                    this.ws.send(JSON.stringify({ type: 'stop_session' }));
                }
                
                this.updateUI();
            }
            
            async sendAudioChunk(audioBlob) {
                if (!this.ws || this.ws.readyState !== WebSocket.OPEN || !this.isActive) return;
                
                try {
                    const arrayBuffer = await audioBlob.arrayBuffer();
                    const audioArray = Array.from(new Uint8Array(arrayBuffer));
                    
                    this.ws.send(JSON.stringify({
                        type: 'audio_chunk',
                        data: audioArray,
                        timestamp: Date.now()
                    }));
                    
                } catch (error) {
                    console.error('–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –∞—É–¥–∏–æ:', error);
                }
            }
            
            handleMessage(data) {
                switch (data.type) {
                    case 'session_ready':
                        this.status.textContent = '–°–ª—É—à–∞—é... –ì–æ–≤–æ—Ä–∏—Ç–µ –≤ –ª—é–±–æ–µ –≤—Ä–µ–º—è!';
                        break;
                        
                    case 'state_changed':
                        this.currentState = data.new_state;
                        this.updateUI();
                        this.updateStatus(data.new_state);
                        break;
                        
                    case 'audio_status':
                        this.updateAudioVisualizer(data);
                        break;
                        
                    case 'speech_detection':
                        if (data.type === 'started') {
                            this.status.textContent = 'üé§ –°–ª—ã—à—É –≤–∞—Å...';
                        } else if (data.type === 'ended') {
                            this.status.textContent = data.will_process ? 
                                '‚öôÔ∏è –û–±—Ä–∞–±–∞—Ç—ã–≤–∞—é...' : '‚è≥ –°–ª–∏—à–∫–æ–º –∫–æ—Ä–æ—Ç–∫–æ, —Å–ª—É—à–∞—é –¥–∞–ª—å—à–µ...';
                        }
                        break;
                        
                    case 'transcription':
                        this.addMessage('user', data.text);
                        break;
                        
                    case 'llm_response':
                        this.addMessage('assistant', data.text);
                        break;
                        
                    case 'processing_complete':
                        this.totalExchanges = data.total_exchanges || this.totalExchanges + 1;
                        this.exchangeCount.textContent = this.totalExchanges;
                        break;
                        
                    case 'interrupted':
                        this.totalInterruptions = data.interruptions_count || this.totalInterruptions + 1;
                        this.interruptCount.textContent = this.totalInterruptions;
                        this.status.textContent = '‚ö° –ü–µ—Ä–µ–±–∏–ª–∏! –°–ª—É—à–∞—é –≤–∞—Å...';
                        break;
                        
                    case 'tts_start':
                        this.status.textContent = 'üîä –ì–æ–≤–æ—Ä—é... (–º–æ–∂–µ—Ç–µ –ø–µ—Ä–µ–±–∏—Ç—å)';
                        break;
                        
                    case 'error':
                        this.status.textContent = '‚ùå ' + data.message;
                        break;
                }
            }
            
            updateStatus(state) {
                const statusMap = {
                    'listening': 'üëÇ –°–ª—É—à–∞—é...',
                    'speech_detected': 'üé§ –°–ª—ã—à—É —Ä–µ—á—å...',
                    'processing': '‚öôÔ∏è –î—É–º–∞—é...',
                    'speaking': 'üîä –û—Ç–≤–µ—á–∞—é...',
                    'interrupted': '‚ö° –ü–µ—Ä–µ–±–∏–ª–∏!',
                    'paused': '‚è∏Ô∏è –ù–∞ –ø–∞—É–∑–µ'
                };
                
                if (statusMap[state]) {
                    this.status.textContent = statusMap[state];
                }
            }
            
            updateAudioVisualizer(data) {
                const amplitude = data.smooth_amplitude || 0;
                const percentage = Math.min(amplitude * 1000, 100); // –£—Å–∏–ª–∏–≤–∞–µ–º –¥–ª—è –≤–∏–∑—É–∞–ª–∏–∑–∞—Ü–∏–∏
                this.audioBar.style.width = percentage + '%';
            }
            
            updateUI() {
                this.mainButton.className = 'main-button';
                
                if (!this.isActive) {
                    this.mainButton.textContent = 'üìû';
                    this.status.textContent = '–ù–∞–∂–º–∏—Ç–µ –¥–ª—è –Ω–∞—á–∞–ª–∞ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞';
                } else {
                    switch (this.currentState) {
                        case 'listening':
                        case 'speech_detected':
                            this.mainButton.classList.add('active');
                            this.mainButton.textContent = 'üé§';
                            break;
                        case 'processing':
                            this.mainButton.classList.add('processing');
                            this.mainButton.textContent = '‚öôÔ∏è';
                            break;
                        case 'speaking':
                            this.mainButton.classList.add('speaking');
                            this.mainButton.textContent = 'üîä';
                            break;
                        default:
                            this.mainButton.classList.add('active');
                            this.mainButton.textContent = 'üìû';
                    }
                }
            }
            
            addMessage(role, text) {
                if (this.conversation.innerHTML.includes('–ù–∞—á–Ω–∏—Ç–µ —Ä–∞–∑–≥–æ–≤–æ—Ä')) {
                    this.conversation.innerHTML = '';
                }
                
                const messageDiv = document.createElement('div');
                messageDiv.className = `message ${role}`;
                messageDiv.textContent = text;
                
                this.conversation.appendChild(messageDiv);
                this.conversation.scrollTop = this.conversation.scrollHeight;
            }
            
            togglePause() {
                this.isPaused = !this.isPaused;
                this.pauseBtn.classList.toggle('active', this.isPaused);
                this.pauseBtn.textContent = this.isPaused ? '‚ñ∂Ô∏è –í–æ–∑–æ–±–Ω–æ–≤–∏—Ç—å' : '‚è∏Ô∏è –ü–∞—É–∑–∞';
                
                if (this.ws && this.ws.readyState === WebSocket.OPEN) {
                    this.ws.send(JSON.stringify({
                        type: this.isPaused ? 'pause_session' : 'resume_session'
                    }));
                }
            }
            
            toggleMute() {
                // –†–µ–∞–ª–∏–∑–∞—Ü–∏—è –æ—Ç–∫–ª—é—á–µ–Ω–∏—è –∑–≤—É–∫–∞
                console.log('Mute toggle - –ø–æ–∫–∞ –Ω–µ —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–æ');
            }
        }
        
        // –ó–∞–ø—É—Å–∫ –ø—Ä–∏–ª–æ–∂–µ–Ω–∏—è
        document.addEventListener('DOMContentLoaded', () => {
            window.assistant = new HandsFreeVoiceAssistant();
        });
    </script>
</body>
</html>
    """)

@app.get("/api/health")
async def health_check():
    return JSONResponse({
        "status": "healthy",
        "version": "6.0.0",
        "description": "Hands-Free Voice Assistant",
        "active_sessions": session_manager.get_active_sessions_count(),
        "features": ["continuous_listening", "interruption_support", "echo_suppression"]
    })

@app.websocket("/ws/hands-free")
async def websocket_hands_free_endpoint(websocket: WebSocket):
    """
    –ì–ª–∞–≤–Ω—ã–π WebSocket endpoint –¥–ª—è hands-free —Ä–µ–∂–∏–º–∞
    """
    await websocket.accept()
    
    session = await session_manager.create_session(websocket)
    
    try:
        while True:
            message = await websocket.receive_json()
            message_type = message.get("type")
            
            if message_type == "audio_chunk":
                # –ù–µ–ø—Ä–µ—Ä—ã–≤–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –∞—É–¥–∏–æ
                audio_data = bytes(message["data"])
                await session.process_audio_chunk(audio_data)
                
            elif message_type == "pause_session":
                await session.pause_session()
                
            elif message_type == "resume_session":
                await session.resume_session()
                
            elif message_type == "stop_session":
                break
            
            else:
                logger.warning(f"–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Ç–∏–ø —Å–æ–æ–±—â–µ–Ω–∏—è: {message_type}")
    
    except WebSocketDisconnect:
        logger.info(f"–ö–ª–∏–µ–Ω—Ç –æ—Ç–∫–ª—é—á–∏–ª—Å—è: {session.session_id}")
    except Exception as e:
        logger.error(f"WebSocket –æ—à–∏–±–∫–∞: {e}")
    finally:
        await session_manager.close_session(session.session_id)

def main():
    logger.info("üöÄ –ó–∞–ø—É—Å–∫ Hands-Free Voice Assistant v6.0")
    logger.info("üìã –†–µ–∂–∏–º '—Ç–µ–ª–µ—Ñ–æ–Ω–Ω–æ–≥–æ —Ä–∞–∑–≥–æ–≤–æ—Ä–∞':")
    logger.info("   - –ü–æ—Å—Ç–æ—è–Ω–Ω–∞—è –ø—Ä–æ—Å–ª—É—à–∫–∞ –º–∏–∫—Ä–æ—Ñ–æ–Ω–∞")
    logger.info("   - –ê–≤—Ç–æ–º–∞—Ç–∏—á–µ—Å–∫–æ–µ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ä–µ—á–∏")
    logger.info("   - –ü–æ–¥–¥–µ—Ä–∂–∫–∞ –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è")
    logger.info("   - –≠—Ö–æ-–ø–æ–¥–∞–≤–ª–µ–Ω–∏–µ")
    logger.info(f"   - VAD –ø–æ—Ä–æ–≥: {HANDS_FREE_CONFIG['vad_threshold']}")
    logger.info(f"   - –ü–æ—Ä–æ–≥ –ø–µ—Ä–µ–±–∏–≤–∞–Ω–∏—è: {HANDS_FREE_CONFIG['interrupt_threshold']}")
    
    port = int(os.getenv("PORT", 10000))
    uvicorn.run(
        app,
        host="0.0.0.0",
        port=port,
        log_level="info"
    )

if __name__ == "__main__":
    main()
